#Evaluating Fine tuned ViT Model

import numpy as np
import torch
from sklearn.metrics import (precision_recall_fscore_support, accuracy_score, roc_curve, auc, 
                             confusion_matrix, matthews_corrcoef, cohen_kappa_score, jaccard_score)

def evaluate_class_metrics(model, dataloader, device, class_names):
    model.eval()
    test_labels, test_predictions = [], []
    all_probs = []

    with torch.no_grad():
        for data, label in dataloader:
            data, label = data.to(device), label.to(device)
            output = model(data)
            _, preds = torch.max(output, 1)
            test_labels.extend(label.cpu().numpy())
            test_predictions.extend(preds.cpu().numpy())
            all_probs.extend(torch.softmax(output, dim=1).cpu().numpy())  # Assuming the model output is logits

    # Convert lists to numpy arrays
    test_labels = np.array(test_labels)
    test_predictions = np.array(test_predictions)
    all_probs = np.array(all_probs)

    # Calculate precision, recall, and f1-score for each class
    precision, recall, f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average=None)

    # Calculate overall precision, recall, and f1-score
    overall_precision, overall_recall, overall_f1, _ = precision_recall_fscore_support(test_labels, test_predictions, average='weighted')

    # Calculate accuracy
    accuracy = accuracy_score(test_labels, test_predictions)
    
    # Calculate confusion matrix
    conf_matrix = confusion_matrix(test_labels, test_predictions)

    # Calculate Matthews Correlation Coefficient
    mcc = matthews_corrcoef(test_labels, test_predictions)

    # Calculate Cohen's Kappa
    kappa = cohen_kappa_score(test_labels, test_predictions)

    # Calculate Specificity for each class
    specificity = []
    for i in range(len(class_names)):
        tn = conf_matrix[i, i]
        fp = conf_matrix.sum(axis=0)[i] - tn
        specificity.append(tn / (tn + fp))
    specificity = np.array(specificity)

    # Calculate Jaccard index for each class
    jaccard = jaccard_score(test_labels, test_predictions, average=None)

    # Calculate overall Jaccard index
    overall_jaccard = jaccard_score(test_labels, test_predictions, average='weighted')

    return (precision, recall, f1, overall_precision, overall_recall, overall_f1, 
            accuracy, conf_matrix, test_labels, all_probs, mcc, kappa, specificity, jaccard, overall_jaccard)

# Evaluate class metrics on the test dataset
results = evaluate_class_metrics(model, test_loader, device, class_names)
(precision, recall, f1, overall_precision, overall_recall, overall_f1, 
 accuracy, conf_matrix, test_labels, all_probs, mcc, kappa, specificity, jaccard, overall_jaccard) = results

# Print the results for each class
print("\nMetrics per class:")
for i, class_name in enumerate(class_names):
    print(f"Class: {class_name}")
    print(f"  Precision: {precision[i]:.4f}")
    print(f"  Recall:    {recall[i]:.4f}")
    print(f"  F1 Score:  {f1[i]:.4f}")
    print(f"  Specificity: {specificity[i]:.4f}")
    print(f"  Jaccard: {jaccard[i]:.4f}")

# Print overall precision, recall, F1-score, accuracy, MCC, Kappa, and Jaccard
print("\nOverall metrics:")
print(f"Precision: {overall_precision:.4f}")
print(f"Recall:    {overall_recall:.4f}")
print(f"F1 Score:  {overall_f1:.4f}")
print(f"Accuracy:  {accuracy:.4f}")
print(f"MCC:       {mcc:.4f}")
print(f"Kappa:     {kappa:.4f}")
print(f"Jaccard:   {overall_jaccard:.4f}")

# Print confusion matrix
print("\nConfusion Matrix:")
print(conf_matrix)

# Plot ROC Curve
import matplotlib.pyplot as plt

plt.figure(figsize=(10, 8))
for i in range(len(class_names)):
    fpr, tpr, _ = roc_curve(test_labels == i, all_probs[:, i])
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{class_names[i]} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend(loc="lower right")
plt.show()
